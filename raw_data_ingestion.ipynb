{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "838c140e-5643-417e-a708-c0103076176e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"Execution_date\", \"\", \"Execution Date (YYYY-MM-DD)\")\n",
    "execution_date = dbutils.widgets.get(\"Execution_date\")\n",
    "execution_date=int(execution_date.replace('-','').replace(\"'\",\"\"))\n",
    "print(f\"Running ETL for execution date: {execution_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b9740e-2276-4823-963c-4cf22251c959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "order_file_path=f'/Volumes/e_comm_data_modeling/default/bronze_e_comm_data/order_{execution_date}.csv'\n",
    "\n",
    "# selling_file_path='/Volumes/e_comm_data_modeling/default/bronze_e_comm_data/bronze_sellers.csv'\n",
    "#Celler data already inserted so no need to read again and again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ece5e9-7571-41e3-a33f-41665c0e1d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_order_df=spark.read.option('header',True)\\\n",
    "        .option('inferschema',True).csv(order_file_path) \n",
    "# raw_seller_df=spark.read.option('header',True)\\\n",
    "#     .option('inferschema',True).csv(selling_file_path) \n",
    "raw_order_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39557c27-0066-43e7-862d-5c161d8405ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# raw_seller_df.printSchema()\n",
    "raw_order_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94ffa6a2-72c2-478e-82fc-05ca40b6bd46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple validations for bronze layer\n",
    "from pyspark.sql import functions as F\n",
    "raw_order_df=raw_order_df.dropna(how=\"all\")\\\n",
    "    .filter(F.col('order_id').isNotNull() & F.col('customer_id').isNotNull() & F.col('seller_id').isNotNull() & F.col('address_id').isNotNull())\n",
    "\n",
    "raw_order_df=raw_order_df.withColumn('order_date',\n",
    "                                     F.when(F.col('order_date').rlike(r'^\\d{4}-\\d{2}-\\d{2}'),\n",
    "                                            F.to_date(F.col('order_date'),'yyyy-MM-dd'))\\\n",
    "                                                .when(F.col('order_date').rlike(r'^\\d{2}-\\d{2}-\\d{4}'),\n",
    "                                                      F.to_date(F.col('order_date'),'dd-MM-yyyy'))\n",
    "                                                          .otherwise(None))\n",
    "\n",
    "raw_order_df=raw_order_df.withColumn('delivery_date',\n",
    "                                     F.when(F.col('delivery_date').rlike(r'^\\d{4}-\\d{2}-\\d{2}'),\n",
    "                                            F.to_date(F.col('delivery_date'),'yyyy-MM-dd'))\\\n",
    "                                                .when(F.col('delivery_date').rlike(r'^\\d{2}-\\d{2}-\\d{4}'),\n",
    "                                                      F.to_date(F.col('delivery_date'),'dd-MM-yyyy'))\\\n",
    "                                                          .otherwise(None))\n",
    "\n",
    "# raw_seller_df=raw_seller_df.dropna(how=\"all\")\\\n",
    "#     .filter(F.col('seller_id').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340fb65c-f374-4773-8d7f-39720b625929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# raw_seller_df.printSchema()\n",
    "# raw_order_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1acc31-c210-4094-8b6d-4a68716784ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Will craete a delta table which will hold daily order \n",
    "-- here we are going to append the data daily by partitioning the data by order_date\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS e_comm_data_modeling.default.raw_order (\n",
    "  order_id STRING,\n",
    "  product_id STRING,\n",
    "  product_name STRING,\n",
    "  category STRING,\n",
    "  seller_id STRING,\n",
    "  customer_id STRING,\n",
    "  customer_name STRING,\n",
    "  address_id STRING,\n",
    "  order_date DATE,\n",
    "  delivery_date DATE,\n",
    "  quantity INT,\n",
    "  unit_price INT,\n",
    "  discount INT,\n",
    "  is_cancelled INT,\n",
    "  is_returned INT\n",
    ") \n",
    "USING DELTA\n",
    "TBLPROPERTIES(\n",
    "  quality = 'bronze',\n",
    "  partitioned_by = 'order_date'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13e170df-7648-4f58-b3ba-0ee1f651029a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_order_df.write.format('delta').mode('append').partitionBy('order_date').saveAsTable('e_comm_data_modeling.default.raw_order')\n",
    "# raw_seller_df.write.format('delta').mode('overwrite').saveAsTable('e_comm_data_modeling.default.seller_data')\n",
    "\n",
    "#.save()  -- it will only store the data will not keep any metadata about the file\n",
    "#.saveasTable --it  will create the table and store into it and will maintain the metadata in metastore\n",
    "#.insertInto() -- if u created the table already and u want to insert the data into the table go for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d499625-3524-47bf-96d4-3d7c3f1ad66b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# raw_order_df.select('order_date').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40107128-bdeb-49d5-a65c-82ec5dc8c132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from delta.tables import *\n",
    "\n",
    "# deltaTable = DeltaTable.forName(spark, 'e_comm_data_modeling.default.raw_order')\n",
    "\n",
    "# fullHistoryDF = deltaTable.history()    # get the full history of the table\n",
    "# fullHistoryDF.show()\n",
    "# lastOperationDF = deltaTable.history(1) \n",
    "# lastOperationDF.show()\n",
    "# df=spark.read.format('delta').option('versionAsOf', 2).table('e_comm_data_modeling.default.raw_order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1c960b-161b-4325-9816-592f2d1c9087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select order_date,count(1) from e_comm_data_modeling.default.raw_order\n",
    "group by order_date;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8708610138242261,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
