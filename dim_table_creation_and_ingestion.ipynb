{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9516ea1-1253-477e-b963-521c97a52082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#All Required Import's  \n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a35f504-00b0-43a3-8cfd-cc943c3536b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#=====================================================RAW TBALE LOADING===================================================\n",
    "raw_table=spark.read.format('delta').table('e_comm_data_modeling.default.raw_order')\n",
    "#========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a19ecbd4-fbbe-4084-9160-e73ed7636fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================================================================\n",
    "#Customer dim table \n",
    "customer_source=raw_table\\\n",
    "    .select(['customer_id','customer_name'])\\\n",
    "        .dropna(how='all')\\\n",
    "            .distinct()\n",
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS e_comm_data_modeling.dim_fact_data.customer(\n",
    "customer_id string ,\n",
    "customer_name string )\n",
    "USING DELTA \n",
    "TBLPROPERTIES(QUALITY='SILVER')\n",
    "''')\n",
    "\n",
    "# Ensure the target table is a Delta table\n",
    "#SCD Type 1\n",
    "customer_table = 'e_comm_data_modeling.dim_fact_data.customer'\n",
    "cust_target = DeltaTable.forName(spark, customer_table)\n",
    "\n",
    "\n",
    "merge_cond = 'target.customer_id = source.customer_id'\n",
    "cust_target.alias('target').merge(\n",
    "    customer_source.alias('source'),merge_cond)\\\n",
    "        .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#=========================================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a683b18-bdfd-43a5-87ea-8040f681b44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================================================================\n",
    "#product dim table \n",
    "product_source=raw_table\\\n",
    "    .select(['product_id','product_name','category'])\\\n",
    "        .dropna(how='all')\\\n",
    "            .distinct()\n",
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS e_comm_data_modeling.dim_fact_data.product(\n",
    "product_id string ,\n",
    "product_name string,\n",
    "category string )\n",
    "USING DELTA \n",
    "TBLPROPERTIES(QUALITY='SILVER')\n",
    "''')\n",
    "\n",
    "\n",
    "# Ensure the target table is a Delta table\n",
    "#SCD Type 1\n",
    "product_table = 'e_comm_data_modeling.dim_fact_data.product'\n",
    "product_target = DeltaTable.forName(spark, product_table)\n",
    "\n",
    "\n",
    "merge_cond = 'target.product_id = source.product_id'\n",
    "product_target.alias('target').merge(\n",
    "    product_source.alias('source'),merge_cond)\\\n",
    "        .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "#=========================================================================================================================        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d45fb2-f724-4ccd-9757-792beeb2e75c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================================================================\n",
    "#adress dim table \n",
    "address_source=raw_table\\\n",
    "    .select(['customer_id','address_id'])\\\n",
    "        .dropna(how='all')\\\n",
    "            .distinct()\n",
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS e_comm_data_modeling.dim_fact_data.address(\n",
    "customer_id string ,\n",
    "address_id string\n",
    ")\n",
    "USING DELTA \n",
    "TBLPROPERTIES(QUALITY='SILVER')\n",
    "''')\n",
    "\n",
    "\n",
    "# Ensure the target table is a Delta table\n",
    "#SCD Type 1\n",
    "address_table = 'e_comm_data_modeling.dim_fact_data.address'\n",
    "address_target = DeltaTable.forName(spark, address_table)\n",
    "\n",
    "\n",
    "merge_cond = 'target.customer_id = source.customer_id and target.address_id = source.address_id'\n",
    "address_target.alias('target').merge(\n",
    "    address_source.alias('source'),merge_cond)\\\n",
    "        .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "#========================================================================================================================        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4770e7a-6b51-4588-936e-62ce1041fe78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================================================================\n",
    "#seller dim table \n",
    "seller_source=raw_table\\\n",
    "    .select(['seller_id','product_id'])\\\n",
    "        .dropna(how='all')\\\n",
    "            .distinct()\n",
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS e_comm_data_modeling.dim_fact_data.seller(\n",
    "seller_id string ,\n",
    "product_id string\n",
    ")\n",
    "USING DELTA \n",
    "TBLPROPERTIES(QUALITY='SILVER')\n",
    "''')\n",
    "\n",
    "\n",
    "# Ensure the target table is a Delta table\n",
    "#SCD Type 1\n",
    "seller_table = 'e_comm_data_modeling.dim_fact_data.seller'\n",
    "seller_target = DeltaTable.forName(spark, seller_table)\n",
    "\n",
    "\n",
    "merge_cond = 'target.seller_id = source.seller_id and target.product_id = source.product_id'\n",
    "seller_target.alias('target').merge(\n",
    "    seller_source.alias('source'),merge_cond)\\\n",
    "        .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "#========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d7cdc9-85f6-441e-863b-fa7eda549f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#=============================================================DATE DIM===================================================\n",
    "date_source1=raw_table.select('order_date').dropna().distinct()\n",
    "date_source2=raw_table.select('delivery_date').dropna().distinct()\n",
    "\n",
    "date_source=date_source1.union(date_source2)\n",
    "\n",
    "#seller dim table \n",
    "date_source1=raw_table.select('order_date').dropna().distinct()\n",
    "date_source2=raw_table.select('delivery_date').dropna().distinct()\n",
    "\n",
    "date_source=date_source1.union(date_source2)\n",
    "\n",
    "date_source=date_source.withColumn('date_key',F.regexp_replace(F.col('order_date'),'-','').cast('int'))\\\n",
    "    .withColumn('full_date',F.to_date(F.col('order_date'),'yyyy-MM-dd'))\\\n",
    "        .withColumn('day',F.dayofmonth(F.col('full_date')))\\\n",
    "            .withColumn('month',F.month(F.col('full_date')))\\\n",
    "                .withColumn('year',F.year(F.col('full_date')))\\\n",
    "                    .withColumn('weekday',F.dayofweek(F.col('full_date')))\\\n",
    "                        .withColumn('is_weekend',F.dayofweek(F.col('full_date')).isin([1,7]).cast('int'))\\\n",
    "                            .drop('order_date')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d133e4-1690-4354-acb0-7e1a9b191207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS e_comm_data_modeling.dim_fact_data.date_table(\n",
    "date_key integer ,\n",
    "full_date date ,\n",
    "day integer ,\n",
    "month integer ,\n",
    "year integer ,\n",
    "weekday integer ,\n",
    "is_weekend integer\n",
    "\n",
    ")\n",
    "USING DELTA \n",
    "TBLPROPERTIES(QUALITY='SILVER')\n",
    "''')\n",
    "\n",
    "\n",
    "# Ensure the target table is a Delta table\n",
    "#SCD Type 1\n",
    "date_table = 'e_comm_data_modeling.dim_fact_data.date_table'\n",
    "date_target = DeltaTable.forName(spark, date_table)\n",
    "\n",
    "merge_cond = 'target.date_key = source.date_key'\n",
    "date_target.alias('target').merge(\n",
    "    date_source.alias('source'),merge_cond)\\\n",
    "        .whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#========================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e243ef97-b897-4031-9913-2b342af79b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql('''select * from e_comm_data_modeling.dim_fact_data.date_table''').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d632efa0-0bf7-4c4f-b580-8334726cc8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================================================================\n",
    "#adress FACT table \n",
    "orders_source=raw_table\\\n",
    "    .select(*['order_id', 'product_id', 'seller_id', 'customer_id',\n",
    " 'address_id', 'order_date', 'delivery_date', 'quantity',\n",
    " 'unit_price', 'discount', 'is_cancelled', 'is_returned']\n",
    ").dropna(how='all')\\\n",
    "    .distinct()\n",
    "\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# wind_spec = Window.partitionBy(F.col('order_id'), F.col('product_id'), F.col('customer_id')).orderBy(F.col('order_id'), F.col('product_id'), F.col('customer_id'))\n",
    "\n",
    "# orders_source=orders_source.withColumn('rn',F.row_number().over(wind_spec))\n",
    "# orders_source.show()\n",
    "# i was trying exclude duplicates but if we do that will lose the cancellation data, so let it be will just append\n",
    "\n",
    "orders_source=orders_source.withColumn('order_date',F.to_date(F.col('order_date'),'yyyy-MM-dd'))\\\n",
    "    .withColumn('delivery_date',F.to_date(F.col('delivery_date'),'yyyy-MM-dd'))\\\n",
    "        .withColumn('delivery_date',F.to_date(F.col('delivery_date'),'yyyy-MM-dd'))\\\n",
    "            .withColumn('duration',F.datediff(F.col('order_date'),F.when(F.col('delivery_date').isNotNull(),F.col('delivery_date')).otherwise(F.col('order_date'))))\\\n",
    "                .withColumn('duration',F.abs(F.col('duration')))\\\n",
    "                    .withColumn('total_amount',(F.col('quantity')*F.col('unit_price'))-F.col('discount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88070f49-22bf-4a0d-82aa-84146a503b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "CREATE TABLE IF NOT EXISTS e_comm_data_modeling.dim_fact_data.fact_order_item(\n",
    "order_id  string \t ,\n",
    "product_id  string   ,\n",
    "seller_id  string    ,\n",
    "customer_id  string  ,\n",
    "address_id  string   ,\n",
    "order_date  date     ,\n",
    "delivery_date  date  ,\n",
    "quantity  int        ,\n",
    "unit_price  int      ,\n",
    "discount  int        ,\n",
    "is_cancelled  int    , \n",
    "is_returned  int     ,\n",
    "duration  int        ,\n",
    "total_amount  int \n",
    ")\n",
    "USING DELTA \n",
    "TBLPROPERTIES(QUALITY='SILVER')\n",
    "''')\n",
    "\n",
    "orders_source.write.mode('append').insertInto('e_comm_data_modeling.dim_fact_data.fact_order_item')\n",
    "# here we are not using any merge because of below reason \n",
    "# -- SCD Type 2 can be applied on the fact table to track changes like delivery status over time.\n",
    "# -- However, for now, we'll use an append-only strategy since the current dataset already includes \n",
    "# -- flags for cancellation and returns, which are sufficient for downstream analysis.\n",
    "\n",
    "# Action\tStatus\n",
    "# Use append-only?\t✅ Yes\n",
    "# Include status flags?\t✅ Already present (is_cancelled, etc.)\n",
    "# Apply SCD Type 2 now?\t❌ Skip for now (adds complexity)\n",
    "\n",
    "# Later, if your use case expands (e.g., order status lifecycle tracking), you can revisit SCD2 logic for the fact.\n",
    "# ========================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea300873-20da-4909-86e8-0fafadeeef3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "096f3109-a936-4184-a458-7f303ef890b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dim_table_creation_and_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
